---
title: 第5章 决策树
categories:
  - machine learning
date: 2017-6-22 11:49:22
tags:
---

**决策树(decision tree)**是一种基本的分类与回归方法。本章主要讨论分类的决策树。

## 5\.1 决策树模型与学习

### 5\.1.1 决策树模型

**定义 5.1(决策树)** 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。

用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。

下图是一个决策树示意图。图中圆和方框分别表示内部结点和叶结点:  
![决策树模型][1]

### 5\.1.2 决策树与if-then规则

可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。即，每一个实例都被一条路径或规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。

### 5\.1.3 决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分(partition)上。将特征空间划分为互不相交的单元(cell)或区域(region)，并在每一个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，则这个条件概率分布可以表示为$P(Y|X)$。各个结点(单元)上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。

### 5\.1.4 决策树学习

决策树学习，假设给定训练数据集 $$D=\lbrace (x_1, y_1),(x_2, y_2),\cdots, (x_N, y_N) \rbrace$$ 其中，$x_i = (x_i^{(1)}, x_i^{(2)},\cdots, x_i^{(n)})^T$为输入实例(特征向量)，$n$为特征个数，$y_i\in\lbrace 1,2,\cdots, K \rbrace$为类标记，$i=1,2,\cdots,N$，$N$为样本容量。学习的目标是根据给定的训练数据集构建一个决策模型，使它能够对实例进行正确的分类。

决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树可能有多个，也可能一个没有。我们需要的是一个与训练数据集矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。

决策树学习用损失函数表示这一目标。决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。

当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优问题。这样得到的决策树是次最优(sub-optimal)的。

 [1]: http://oqfqjieze.bkt.clouddn.com/blog/decision_tree_model.png